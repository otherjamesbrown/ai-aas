# Default values for vllm-deployment
# This is a YAML-formatted file.

replicaCount: 1

image:
  repository: vllm/vllm-openai
  tag: "v0.4.0"  # Pin to specific version for reproducibility
  pullPolicy: IfNotPresent

model:
  # Model path or identifier (e.g., "meta-llama/Llama-2-7b-chat-hf")
  path: ""
  # Model size category for timeout configuration
  # Options: small (7B-13B), medium (30B-40B), large (70B+)
  size: "small"

service:
  type: ClusterIP
  port: 8000
  name: http

# GPU resource configuration
resources:
  limits:
    nvidia.com/gpu: 1
    memory: "48Gi"
    cpu: "16"
  requests:
    nvidia.com/gpu: 1
    memory: "32Gi"
    cpu: "8"

# Node selector for GPU node pool
nodeSelector:
  node-type: gpu

# Tolerations for GPU workloads
tolerations:
  - key: gpu-workload
    operator: Equal
    value: "true"
    effect: NoSchedule

# Health check probes
livenessProbe:
  httpGet:
    path: /health
    port: http
  initialDelaySeconds: 60
  periodSeconds: 30
  timeoutSeconds: 5
  failureThreshold: 3

readinessProbe:
  httpGet:
    path: /ready
    port: http
  initialDelaySeconds: 30
  periodSeconds: 10
  timeoutSeconds: 5
  failureThreshold: 3

# Startup probe for model loading
# Timeout calculation: failureThreshold * periodSeconds
# Small models (7B-13B): ~10 minutes max (20 * 30s)
# Medium models (30B-40B): ~15 minutes max (30 * 30s)
# Large models (70B+): ~20 minutes max (40 * 30s)
startupProbe:
  httpGet:
    path: /ready
    port: http
  initialDelaySeconds: 0
  periodSeconds: 30
  timeoutSeconds: 10
  failureThreshold: 20  # Adjust based on model.size

# vLLM configuration
vllm:
  host: "0.0.0.0"
  port: 8000
  # Additional vLLM environment variables
  env: []

# Service account
serviceAccount:
  create: true
  name: ""

# Network policy
networkPolicy:
  enabled: true
  # Allow traffic from API Router Service
  # Note: Update namespace to match where API Router Service is deployed
  allowedSources:
    - namespace: system
      podSelector:
        matchLabels:
          app.kubernetes.io/name: api-router-service

# Prometheus monitoring
prometheus:
  enabled: true
  serviceMonitor:
    enabled: true
    interval: 30s
    scrapeTimeout: 10s

# Environment configuration
environment: development
namespace: system

# Labels and annotations
labels: {}
annotations: {}

# Pre-install checks
preInstallChecks:
  enabled: true  # Enable GPU availability check before deployment

